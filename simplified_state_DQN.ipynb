{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fc61c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\for_torch\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import package\n",
    "import functools\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from pettingzoo import ParallelEnv\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "import pandas as pd\n",
    "import pyNetLogo\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import random\n",
    "import math\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pettingzoo.butterfly.pistonball_v6 as pistonball_v6\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95edf4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model\n",
    "\n",
    "net_model = pyNetLogo.NetLogoLink(gui = True, netlogo_home = 'D:\\\\Program Files\\\\NetLogo 6.1.0',netlogo_version = '6.1',jvm_home = 'D:\\\\Program Files\\\\NetLogo 6.1.0\\\\runtime\\\\bin\\\\server\\\\jvm.dll')\n",
    "\n",
    "net_model.load_model('C:\\\\Users\\\\Owner\\\\OneDrive - Delft University of Technology\\\\homework\\\\graduation_work\\\\netlogo model\\\\layout_1_allocation.nlogo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb40a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set order for each parking space\n",
    "def reset_state():\n",
    "    global info_vehicle\n",
    "    global num_last_vehicles\n",
    "    global car_total_number\n",
    "    net_model.command('setup')\n",
    "    net_model.command('ask patches [set order plabel]')\n",
    "    car_total_number = net_model.report('car-total-number')\n",
    "    order = net_model.patch_report('order')\n",
    "    sequence_0_1 = np.array([[35, 7, 24, 43, 31, 11],\n",
    "                           [14, 12, 13, 4, 1, 2]])\n",
    "    sequence_0_2 = np.array([[30, 8, 41, 34, 27, 44],\n",
    "                            [39, 0, 22, 17, 32, 20]])\n",
    "    sequence_0_3 = np.array([[6, 21, 5, 25, 15, 23],\n",
    "                            [28, 36, 46, 47, 45, 18]])\n",
    "    sequence_0_4 =np.array([[33, 10, 3, 26, 9, 38],\n",
    "                           [29, 16, 19, 40, 37, 42]] )\n",
    "    order.iloc[-4:-2,9:15] = sequence_0_1 + 1 \n",
    "    order.iloc[-4:-2, 2:8 ] = sequence_0_2 + 1\n",
    "    order.iloc[3:5, 2:8] = sequence_0_3 + 1\n",
    "    order.iloc[3:5,9:15] = sequence_0_4 + 1\n",
    "    # order is from 1  to the car_total_number\n",
    "    net_model.patch_set('order',order)\n",
    "    net_model.command('ask patches with [order = 0] [set order -1]')\n",
    "    net_model.command('ask patches [set plabel (plabel - 2)]')\n",
    "    net_model.patch_report('order')\n",
    "    net_model.command('ask patches with [pcolor = grey][set plabel -1]')\n",
    "    \n",
    "    #info about the parking situation about vehicles\n",
    "    who = np.array(range(int(car_total_number)))\n",
    "    parked_or_not = np.zeros(int(car_total_number))\n",
    "    time_spent = np.zeros(int(car_total_number))\n",
    "    rewarded = np.zeros(int(car_total_number))\n",
    "    info_vehicle = np.array([who, parked_or_not, time_spent, rewarded]).T\n",
    "    info_vehicle = pd.DataFrame(info_vehicle, columns = ['who', 'parked_or_not', 'time_spent','rewarded'])\n",
    "    num_last_vehicles = 0\n",
    "    \n",
    "    \n",
    "reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9523efe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_total_number = net_model.report('car-total-number')\n",
    "space_number = net_model.report('count patches with [pcolor = grey]')\n",
    "space_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d6e7c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1)\n"
     ]
    }
   ],
   "source": [
    "def reset_state():\n",
    "    global info_vehicle\n",
    "    global num_last_vehicles\n",
    "    global car_total_number\n",
    "    net_model.command('setup')\n",
    "    net_model.command('ask patches [set order plabel]')\n",
    "    order = net_model.patch_report('order')\n",
    "    sequence_0_1 = 48 - np.array([[25, 27, 29, 31, 33, 35],\n",
    "                       [6, 5, 4, 3, 2, 1]])\n",
    "    sequence_0_2 = 48 - np.array([[13, 15, 17, 19, 21, 23],\n",
    "                            [12, 11, 10, 9, 8, 7]])\n",
    "    sequence_0_3 = 48 - np.array([[14, 16, 18, 20, 22, 24],\n",
    "                            [48, 47, 46, 45, 44, 43]])\n",
    "    sequence_0_4 = 48 - np.array([[26, 28, 30, 32, 34, 36],\n",
    "                           [42, 41, 40, 39, 38, 37]] )\n",
    "    order.iloc[-4:-2,9:15] = sequence_0_1 + 1 \n",
    "    order.iloc[-4:-2, 2:8 ] = sequence_0_2 + 1\n",
    "    order.iloc[3:5, 2:8] = sequence_0_3 + 1\n",
    "    order.iloc[3:5,9:15] = sequence_0_4 + 1\n",
    "    # order is from 1  to the car_total_number\n",
    "    net_model.patch_set('order',order)\n",
    "    net_model.command('ask patches with [order = 0] [set order -1]')\n",
    "    net_model.command('ask patches [set plabel (plabel - 2)]')\n",
    "    net_model.patch_report('order')\n",
    "    net_model.command('ask patches with [pcolor = grey][set plabel -1]')\n",
    "    #info about the parking situation about vehicle\n",
    "    who = np.array(range(int(car_total_number)))\n",
    "    parked_or_not = np.zeros(int(car_total_number))\n",
    "    time_spent = np.zeros(int(car_total_number))\n",
    "    rewarded = np.zeros(int(car_total_number))\n",
    "    info_vehicle = np.array([who, parked_or_not, time_spent, rewarded]).T\n",
    "    info_vehicle = pd.DataFrame(info_vehicle, columns = ['who', 'parked_or_not', 'time_spent','rewarded'])\n",
    "    num_last_vehicles = 0\n",
    "  \n",
    "    \n",
    "reset_state()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_environment(netlogo):\n",
    "#     num_now = np.array([int(net_model.report('count turtles'))])\n",
    "#     # from 0 to 48\n",
    "                \n",
    "#     return num_now\n",
    "def get_environment(netlogo):\n",
    "    patches = np.zeros(int(car_total_number))\n",
    "    if net_model.report('count turtles') == car_total_number:\n",
    "        patches = np.ones(int(car_total_number))\n",
    "    else:\n",
    "        patches_num = net_model.report('count patches with [plabel > 0]')\n",
    "        if patches_num > 0:\n",
    "            patch_label = set(net_model.report('[order] of patches with [plabel > 0]'))\n",
    "            for item in patch_label:\n",
    "                patches[int(item - 1)] = 1\n",
    "    return patches\n",
    "def get_reward(): # different for each agent, same if they have the same objective\n",
    "    # if 5 more vehicles are parked or all vehicles are parked, get a reward.\n",
    "    global info_vehicle\n",
    "    global num_last_vehicles\n",
    "    N = 1\n",
    "    reward = 0\n",
    "    parked_vehicle_num = net_model.report('count turtles with [parked? = true]')\n",
    "    if parked_vehicle_num > 0:\n",
    "        parked_vehicle = list(net_model.report('[who] of turtles with [parked? = true]')) \n",
    "        #np.arra\n",
    "        for i in parked_vehicle:\n",
    "            row_index = info_vehicle.index[info_vehicle['who'] == i][0]\n",
    "            info_vehicle.at[row_index, 'parked_or_not'] = 1\n",
    "    \n",
    "    info_vehicle.loc[info_vehicle['parked_or_not'] == 0, 'time_spent'] += 1\n",
    "    if (parked_vehicle_num >= (num_last_vehicles + N))  or parked_vehicle_num == car_total_number:\n",
    "        \n",
    "            \n",
    "        # reward should be the total time spend with these newly parked vehicles\n",
    "        # be scaled in [0,100] in most cases, using linear function \n",
    "        avg_time_spent = info_vehicle.loc[(info_vehicle['rewarded'] == 0) & (info_vehicle['parked_or_not'] == 1), 'time_spent'].sum() / (parked_vehicle_num - num_last_vehicles)\n",
    "        info_vehicle.loc[(info_vehicle['rewarded'] == 0) & (info_vehicle['parked_or_not'] == 1), 'rewarded'] = 1\n",
    "        # scale the reward\n",
    "#         print(info_vehicle)\n",
    "\n",
    "        reward = (- avg_time_spent + 100) / (100 - 35) * 100\n",
    "        num_last_vehicles = parked_vehicle_num\n",
    "    return reward\n",
    "\n",
    "def perform_action(action, unava):\n",
    "    'action is the order for the parking space'\n",
    "    'if the action choose this parking space, then it will have a plabel'\n",
    "    'the same with the car entering'\n",
    "    'order from 1 to the parking_space number'\n",
    "    'but action may start from 0, so maybe should modify it later'\n",
    "    if len(unava) == car_total_number:\n",
    "        return\n",
    "    num_turtles = net_model.report('count turtles')\n",
    "    if num_turtles > 0:\n",
    "        \n",
    "#         unava_plabel = set(net_model.report(\"[plabel] of patches with [pcolor = gray]\"))\n",
    "#         who = int(num_turtles - 1)\n",
    "        who = max(set(net_model.report('[plabel] of patches with [pcolor = grey]'))) + 1\n",
    "#         if who not in set(net_model.report('[plabel] of patches with [pcolor = grey]')):\n",
    "        if who < car_total_number:\n",
    "            s = 'ask patches with [order = '+str(int(action + 1))+'][set plabel '+str(int(who))+']'\n",
    "    #         if (who not in unava_plabel):\n",
    "            net_model.command(s)\n",
    "\n",
    "def observe():\n",
    "    # all agents share the same observation\n",
    "    envir = get_environment(net_model).reshape(48,1)\n",
    "    return envir\n",
    "\n",
    "\n",
    "\n",
    "obs = observe() # observe is the same\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602117d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e89e0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(space_number)\n",
    "#         self.observation_space = spaces.Discrete(space_number+1)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=obs.shape, dtype=np.uint8)\n",
    "        self.seed()\n",
    "        self.state = None\n",
    "        self.num_iter = 0\n",
    "        self.reward = 0\n",
    "        self.unava = []\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed],\n",
    "\n",
    "    def reset(self):\n",
    "        reset_state()\n",
    "        self.state = observe()\n",
    "        self.num_iter = 0\n",
    "        self.reward = 0\n",
    "        self.info = {'training': self.reward}\n",
    "        self.action_space = spaces.Discrete(space_number)\n",
    "        self.unava = []\n",
    "        net_model.command('go')\n",
    "\n",
    "        return self.state\n",
    "    def step(self, action):\n",
    "        # perform action\n",
    "        # action is from 0\n",
    "        # order is action + 1\n",
    "        max_iter = 900\n",
    "#         while self.action_ava.contains(action) == false:\n",
    "#             self.step()\n",
    "        who = net_model.report('count turtles')\n",
    "        unava_num = net_model.report('count patches with [plabel > 0]')\n",
    "        if unava_num > 0:\n",
    "            self.unava = list(map(int,set(net_model.report(\"[order] of patches with [plabel >= 0]\"))))\n",
    "        if unava_num == car_total_number:\n",
    "            action = None\n",
    "            while net_model.report('count turtles with [parked = true]') < car_total_number:\n",
    "                net_model.command('go')\n",
    "                self.num_iter += 1\n",
    "\n",
    "            \n",
    "        else:\n",
    "            assert self.action_space.contains(action)\n",
    "#             action= get_action_mask(action)\n",
    "            \n",
    "            perform_action(action, self.unava)\n",
    "            self.unava = list(map(int,set(net_model.report(\"[order] of patches with [plabel >= 0]\"))))\n",
    "            net_model.command('go')\n",
    "            \n",
    "            while net_model.report('count turtles') < who + 1 and who != car_total_number:\n",
    "                net_model.command('go')\n",
    "                self.num_iter += 1\n",
    "\n",
    "        #reward\n",
    "        self.reward = get_reward()\n",
    "\n",
    "        #state\n",
    "        self.state = observe()\n",
    "        terminated = (net_model.report('count turtles with [parked? = true]') == car_total_number)\n",
    "        truncated = (self.num_iter >= max_iter)\n",
    "                    \n",
    "        done = (terminated or truncated)\n",
    "        if done:\n",
    "            self.info = {'final_reward': self.reward}\n",
    "        else:\n",
    "            self.info = {'training': self.reward}\n",
    "\n",
    "        return self.state, self.reward, terminated, truncated, self.info\n",
    "    def close(self):\n",
    "        net_model.kill_workspace()      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b07e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53456790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a841e38c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env = MyCustomEnv()\n",
    "# env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bed8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "321a11c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = MyCustomEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d14c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723c9894",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'namedtuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# reply memory\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Transition \u001b[38;5;241m=\u001b[39m \u001b[43mnamedtuple\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransition\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m                         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_state\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mReplayMemory\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, capacity):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'namedtuple' is not defined"
     ]
    }
   ],
   "source": [
    "# reply memory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "# Q-network\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,48)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training\n",
    "# Hyperparameters and utilities\n",
    "\n",
    "\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the AdamW optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 2000\n",
    "TAU = 0.005\n",
    "LR = 0.01\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state = env.reset()\n",
    "n_observations = len(state)\n",
    "print('n_observations',n_observations)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state,env):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if len(env.unava) == car_total_number:\n",
    "        return torch.tensor(env.action_space.sample()).view(1,1)\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "\n",
    "            for i in torch.sort(policy_net(state),descending = True)[1][0]:\n",
    "                if (int(i) + 1) not in env.unava:\n",
    "                    return i.view(1,1)\n",
    "                \n",
    "\n",
    "#             return policy_net(state).max(1)[1].view(1, 1)\n",
    "                \n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        while int(action + 1) in env.unava:\n",
    "            action = env.action_space.sample()\n",
    "        return torch.tensor(action).view(1,1)\n",
    "#     if sample > eps_threshold:\n",
    "#         with torch.no_grad():\n",
    "#             # t.max(1) will return the largest column value of each row.\n",
    "#             # second column on max result is index of where max element was\n",
    "#             # found, so we pick action with the larger expected reward.\n",
    "#             q_values = policy_net(state)\n",
    "#             q_values = q_values.detach().numpy()\n",
    "#             sorted_indices = np.argsort(-q_values)\n",
    "#             for index in sorted_indices[0]:\n",
    "#                 action = int(index)\n",
    "#                 if action in env.action_space:\n",
    "#                     return torch.tensor([[action]], device=device, dtype=torch.long)\n",
    "\n",
    "#     return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "total_reward = []\n",
    "duration_loss = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(total_reward, dtype=torch.float)\n",
    "#     durations_t = torch.tensor(duration_loss.copy(), dtype=torch.float)\n",
    "    print(durations_t)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "\n",
    "# training loop\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 500\n",
    "else:\n",
    "    num_episodes = 2\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    rew_eps = 0\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state,env)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        loss = optimize_model()\n",
    "        \n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        rew_eps += float(reward)     \n",
    "        if done:\n",
    "            total_reward.append(rew_eps)\n",
    "            duration_loss.append(float(loss))\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c93c7391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.24570461576284"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667a3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy_net(state).max(1)[1].view(1, 1)\n",
    "# for i in torch.sort(policy_net(state),descending = True)[1][0]:\n",
    "#     print(int(i))\n",
    "    \n",
    "# torch.sort(policy_net(state),descending = True)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "951e3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, n_channels, n_actions):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(n_channels, 32, kernel_size=(1,8), stride=4)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=1)\n",
    "#         self.fc1 = nn.Linear(3008, 512)\n",
    "#         self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "    \n",
    "# policy_net = DQN(1,48).to(device)\n",
    "# target_net = DQN(1,48).to(device)\n",
    "# target_net.load_state_dict(policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "898d980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, info = env.reset()\n",
    "# n_observations = len(state)\n",
    "# n_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b46f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bd5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1de3c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# import math\n",
    "# import random\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import namedtuple, deque\n",
    "# from itertools import count\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# # set up matplotlib\n",
    "# is_ipython = 'inline' in matplotlib.get_backend()\n",
    "# if is_ipython:\n",
    "#     from IPython import display\n",
    "\n",
    "# plt.ion()\n",
    "\n",
    "# # if gpu is to be used\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transition = namedtuple('Transition',\n",
    "#                         ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "# class ReplayMemory(object):\n",
    "\n",
    "#     def __init__(self, capacity):\n",
    "#         self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "#     def push(self, *args):\n",
    "#         \"\"\"Save a transition\"\"\"\n",
    "#         self.memory.append(Transition(*args))\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         return random.sample(self.memory, batch_size)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.memory)\n",
    "    \n",
    "# class DQN(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_observations, n_actions):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.layer1 = nn.Linear(n_observations, 128)\n",
    "#         self.layer2 = nn.Linear(128, 128)\n",
    "#         self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "#     # Called with either one element to determine next action, or a batch\n",
    "#     # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.layer1(x))\n",
    "#         x = F.relu(self.layer2(x))\n",
    "#         return self.layer3(x)\n",
    "    \n",
    "# # BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# # GAMMA is the discount factor as mentioned in the previous section\n",
    "# # EPS_START is the starting value of epsilon\n",
    "# # EPS_END is the final value of epsilon\n",
    "# # EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# # TAU is the update rate of the target network\n",
    "# # LR is the learning rate of the AdamW optimizer\n",
    "# BATCH_SIZE = 128\n",
    "# GAMMA = 0.99\n",
    "# EPS_START = 0.9\n",
    "# EPS_END = 0.05\n",
    "# EPS_DECAY = 1000\n",
    "# TAU = 0.005\n",
    "# LR = 1e-4\n",
    "\n",
    "# # Get number of actions from gym action space\n",
    "# n_actions = env.action_space.n\n",
    "# # Get the number of state observations\n",
    "# state, info = env.reset()\n",
    "# n_observations = len(state)\n",
    "\n",
    "# policy_net = DQN(n_observations, n_actions).to(device)\n",
    "# target_net = DQN(n_observations, n_actions).to(device)\n",
    "# target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "# memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "# steps_done = 0\n",
    "\n",
    "\n",
    "# def select_action(state):\n",
    "#     global steps_done\n",
    "#     sample = random.random()\n",
    "#     eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "#         math.exp(-1. * steps_done / EPS_DECAY)\n",
    "#     steps_done += 1\n",
    "#     if sample > eps_threshold:\n",
    "#         with torch.no_grad():\n",
    "#             # t.max(1) will return the largest column value of each row.\n",
    "#             # second column on max result is index of where max element was\n",
    "#             # found, so we pick action with the larger expected reward.\n",
    "#             return policy_net(state).max(1)[1].view(1, 1)\n",
    "#     else:\n",
    "#         return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "# episode_durations = []\n",
    "\n",
    "\n",
    "# def plot_durations(show_result=False):\n",
    "#     plt.figure(1)\n",
    "#     durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "#     if show_result:\n",
    "#         plt.title('Result')\n",
    "#     else:\n",
    "#         plt.clf()\n",
    "#         plt.title('Training...')\n",
    "#     plt.xlabel('Episode')\n",
    "#     plt.ylabel('Duration')\n",
    "#     plt.plot(durations_t.numpy())\n",
    "#     # Take 100 episode averages and plot them too\n",
    "#     if len(durations_t) >= 100:\n",
    "#         means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "#         means = torch.cat((torch.zeros(99), means))\n",
    "#         plt.plot(means.numpy())\n",
    "\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "#     if is_ipython:\n",
    "#         if not show_result:\n",
    "#             display.display(plt.gcf())\n",
    "#             display.clear_output(wait=True)\n",
    "#         else:\n",
    "#             display.display(plt.gcf())\n",
    "            \n",
    "\n",
    "# def optimize_model():\n",
    "#     if len(memory) < BATCH_SIZE:\n",
    "#         return\n",
    "#     transitions = memory.sample(BATCH_SIZE)\n",
    "#     # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "#     # detailed explanation). This converts batch-array of Transitions\n",
    "#     # to Transition of batch-arrays.\n",
    "#     batch = Transition(*zip(*transitions))\n",
    "\n",
    "#     # Compute a mask of non-final states and concatenate the batch elements\n",
    "#     # (a final state would've been the one after which simulation ended)\n",
    "#     non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "#                                           batch.next_state)), device=device, dtype=torch.bool)\n",
    "#     non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "#                                                 if s is not None])\n",
    "#     state_batch = torch.cat(batch.state)\n",
    "#     action_batch = torch.cat(batch.action)\n",
    "#     reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "#     # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "#     # columns of actions taken. These are the actions which would've been taken\n",
    "#     # for each batch state according to policy_net\n",
    "#     state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "#     # Compute V(s_{t+1}) for all next states.\n",
    "#     # Expected values of actions for non_final_next_states are computed based\n",
    "#     # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "#     # This is merged based on the mask, such that we'll have either the expected\n",
    "#     # state value or 0 in case the state was final.\n",
    "#     next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "#     with torch.no_grad():\n",
    "#         next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "#     # Compute the expected Q values\n",
    "#     expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "#     # Compute Huber loss\n",
    "#     criterion = nn.SmoothL1Loss()\n",
    "#     loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "#     # Optimize the model\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     # In-place gradient clipping\n",
    "#     torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "#     optimizer.step()\n",
    "    \n",
    "# if torch.cuda.is_available():\n",
    "#     num_episodes = 600\n",
    "# else:\n",
    "#     num_episodes = 600\n",
    "\n",
    "# for i_episode in range(num_episodes):\n",
    "#     # Initialize the environment and get it's state\n",
    "#     state, info = env.reset()\n",
    "#     state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "#     for t in count():\n",
    "#         action = select_action(state)\n",
    "#         observation, reward, terminated, truncated, _= env.step(action.item())\n",
    "#         reward = torch.tensor([reward], device=device)\n",
    "#         done = terminated or truncated\n",
    "\n",
    "#         if terminated:\n",
    "#             next_state = None\n",
    "#         else:\n",
    "#             next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "#         # Store the transition in memory\n",
    "#         memory.push(state, action, next_state, reward)\n",
    "\n",
    "#         # Move to the next state\n",
    "#         state = next_state\n",
    "\n",
    "#         # Perform one step of the optimization (on the policy network)\n",
    "#         optimize_model()\n",
    "\n",
    "#         # Soft update of the target network's weights\n",
    "#         # θ′ ← τ θ + (1 −τ )θ′\n",
    "#         target_net_state_dict = target_net.state_dict()\n",
    "#         policy_net_state_dict = policy_net.state_dict()\n",
    "#         for key in policy_net_state_dict:\n",
    "#             target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "#         target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "#         if done:\n",
    "#             episode_durations.append(t + 1)\n",
    "#             plot_durations()\n",
    "#             break\n",
    "\n",
    "# print('Complete')\n",
    "# plot_durations(show_result=True)\n",
    "# plt.ioff()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2857b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# # set up matplotlib\n",
    "# is_ipython = 'inline' in matplotlib.get_backend()\n",
    "# if is_ipython:\n",
    "#     from IPython import display\n",
    "\n",
    "# plt.ion()\n",
    "\n",
    "# # if gpu is to be used\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transition = namedtuple('Transition',\n",
    "#                         ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "# class ReplayMemory(object):\n",
    "\n",
    "#     def __init__(self, capacity):\n",
    "#         self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "#     def push(self, *args):\n",
    "#         \"\"\"Save a transition\"\"\"\n",
    "#         self.memory.append(Transition(*args))\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         return random.sample(self.memory, batch_size)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.memory)\n",
    "    \n",
    "# class DQN(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_observations, n_actions):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.layer1 = nn.Linear(n_observations, 128)\n",
    "#         self.layer2 = nn.Linear(128, 128)\n",
    "#         self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "#     # Called with either one element to determine next action, or a batch\n",
    "#     # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.layer1(x))\n",
    "#         x = F.relu(self.layer2(x))\n",
    "#         return self.layer3(x)\n",
    "    \n",
    "# # BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# # GAMMA is the discount factor as mentioned in the previous section\n",
    "# # EPS_START is the starting value of epsilon\n",
    "# # EPS_END is the final value of epsilon\n",
    "# # EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# # TAU is the update rate of the target network\n",
    "# # LR is the learning rate of the AdamW optimizer\n",
    "# BATCH_SIZE = 128\n",
    "# GAMMA = 0.99\n",
    "# EPS_START = 0.9\n",
    "# EPS_END = 0.05\n",
    "# EPS_DECAY = 1000\n",
    "# TAU = 0.005\n",
    "# LR = 1e-4\n",
    "\n",
    "# # Get number of actions from gym action space\n",
    "# n_actions = env.action_space.n\n",
    "# # Get the number of state observations\n",
    "# state, info = env.reset()\n",
    "# n_observations = len(state)\n",
    "# print(state)\n",
    "# print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31b6c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# time.time_ns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e7384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.617290140942288\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "for i in range(48):\n",
    "    gamma *= 0.99\n",
    "print(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db903c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:for_torch]",
   "language": "python",
   "name": "conda-env-for_torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
